---
title: "Dracula — Lemmatized Term-Frequency (Pyodide + precomputed chap_lemmas.csv)"
format: html
execute:
  echo: false
  enabled: true
---
<div style="max-width:900px">
  <p>Enter a single **lemmatized** word (no multi-word phrases). Example: <code>vampire</code></p>

  <label for="lemma">Lemma:</label>
  <input id="lemma" placeholder="vampire" style="width:220px" />
  <button id="plotBtn">Plot</button>
  <label style="margin-left:12px"><input type="checkbox" id="normalize" /> Normalize (per 1k words)</label>

  <!-- status is now an ARIA live region -->
  <div id="status" style="margin-top:8px;color:#666" aria-live="polite" role="status">Status: initializing...</div>
  <div id="plotArea" style="margin-top:12px"></div>

  <p style="color:#666; margin-top:14px; font-size:0.95em">
    If you want full spaCy-powered searches or NER, run the original notebook in Binder/Colab; this page is designed for lightweight in-browser lemmatized lookups using a precomputed CSV.
  </p>
</div>

<script type="module">
/* Unified Pyodide script with accessibility outputs + trend calculation */

const CSV_CANDIDATES = [
  "https://iulibscholcomm.github.io/dracula/data/chap_lemmas.csv",
  "/dracula/data/chap_lemmas.csv",
  "/data/chap_lemmas.csv",
  "data/chap_lemmas.csv",
  "chap_lemmas.csv"
];

const statusEl = document.getElementById("status");
const plotEl = document.getElementById("plotArea");
const plotBtn = document.getElementById("plotBtn");
const lemmaInput = document.getElementById("lemma");
const normalizeBox = document.getElementById("normalize");

let pyodide = null;
let pyReady = false;
let csvTextCache = null;
let pyState = null; // { df_proxy, counters_proxy, source }

// ---------- Python snippets ----------
// Initialization (unchanged except variable names preserved)
const initPyGlobals = `
import io, pandas as pd
from collections import Counter

# csv_text is injected by JS
if 'csv_text' not in globals() or csv_text is None:
    raise ValueError("csv_text not provided or is None.")

# Read CSV robustly
try:
    df = pd.read_csv(io.StringIO(csv_text), encoding='utf-8')
except Exception as e:
    preview = (csv_text or "")[:1000]
    raise ValueError(f"pd.read_csv failed: {e}\\nPreview (first 1000 chars):\\n{preview}")

# Validate minimal columns
required = ['chapter','lemmas_str','word_count']
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError("Missing columns in chap_lemmas.csv: " + ",".join(missing))

# Normalize types and fillna
df['chapter'] = pd.to_numeric(df['chapter'], errors='coerce').fillna(0).astype(int)
df['lemmas_str'] = df.get('lemmas_str', '').fillna("").astype(str)
df['word_count'] = pd.to_numeric(df.get('word_count', 0), errors='coerce').fillna(0).astype(int)

# Build counters per row
chapter_counters = []
for s in df['lemmas_str'].tolist():
    if not s:
        chapter_counters.append(Counter())
    else:
        chapter_counters.append(Counter(s.split()))

# Expose to JS via pyodide.globals
chap_df = df
chapter_counters = chapter_counters

# readiness
"OK"
`;

/* Plot template now returns a tuple: (base64_png, out_json)
   out_json includes numeric arrays and a short trend sentence computed
   using linear slope + Pearson r (numpy). */
const plotPyTemplate = `
import io, base64, matplotlib, json
matplotlib.use("agg")
import matplotlib.pyplot as plt
import numpy as np

# injected by JS: df, counters, query, normalize_flag
q = query.strip().lower()
if not q:
    raise ValueError("Empty query")

# get counts per chapter
try:
    counts = [int(c.get(q, 0)) for c in counters]
except Exception as e:
    raise ValueError("Error iterating counters: " + str(e))

lengths = [int(x) for x in df['word_count'].tolist()]
arr = np.array(counts, dtype=float)

# normalized series (per 1k words) if requested
if normalize_flag:
    lens = np.array(lengths, dtype=float)
    lens[lens==0] = 1.0
    arr_norm = (arr / lens * 1000.0)
    series_for_trend = arr_norm
else:
    arr_norm = None
    series_for_trend = arr

# simple trend calculation using linear regression and Pearson r
trend_text = ""
if len(series_for_trend) >= 2:
    x = np.arange(1, len(series_for_trend)+1, dtype=float)
    # fit linear slope (y = a*x + b)
    a, b = np.polyfit(x, series_for_trend, 1)
    # Pearson correlation
    try:
        r = float(np.corrcoef(x, series_for_trend)[0,1])
    except Exception:
        r = 0.0

    # interpret: thresholds are heuristic
    if abs(r) >= 0.4:
        if a > 0:
            trend_text = f"Increasing trend (slope={a:.3f}, r={r:.2f})."
        else:
            trend_text = f"Decreasing trend (slope={a:.3f}, r={r:.2f})."
    elif abs(r) >= 0.2:
        if a > 0:
            trend_text = f"Mild upward trend (slope={a:.3f}, r={r:.2f})."
        else:
            trend_text = f"Mild downward trend (slope={a:.3f}, r={r:.2f})."
    else:
        trend_text = f"No clear trend (slope={a:.3f}, r={r:.2f})."
else:
    trend_text = "Not enough points to compute trend."

# Plotting (visual for sighted users)
x = np.arange(1, len(arr)+1)
fig, ax = plt.subplots(figsize=(9,3))
ax.plot(x, arr, marker='o', linewidth=1)
ax.set_xlabel("Chapter")
ax.set_ylabel("Occurrences per 1k words" if normalize_flag else "Raw count")
ax.set_title(f"Frequency of '{query}' (lemmatized) — n_chapters={len(arr)}")
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
buf = io.BytesIO()
fig.savefig(buf, format='png', dpi=150)
plt.close(fig)
buf.seek(0)
b64 = base64.b64encode(buf.getvalue()).decode('ascii')

# Prepare numeric summary
arr_raw = arr.tolist()
arr_norm_list = arr_norm.tolist() if arr_norm is not None else None

total = int(np.sum(arr_raw))
max_val = None if len(arr_raw)==0 else int(np.max(arr_raw))
max_chapters = [] if max_val is None else [i+1 for i,v in enumerate(arr_raw) if int(v)==max_val]
min_val = None if len(arr_raw)==0 else int(np.min(arr_raw))
min_chapters = [] if min_val is None else [i+1 for i,v in enumerate(arr_raw) if int(v)==min_val]

out = {
    "query": query,
    "n_chapters": len(arr_raw),
    "counts_raw": arr_raw,
    "counts_norm": arr_norm_list,
    "word_counts": lengths,
    "total_raw": total,
    "max_raw": int(max_val) if max_val is not None else None,
    "max_chapters": max_chapters,
    "min_raw": int(min_val) if min_val is not None else None,
    "min_chapters": min_chapters,
    "trend_text": trend_text
}

out_json = json.dumps(out)
b64, out_json
`;

// ---------- Helpers ----------

async function loadPy() {
  statusEl.textContent = "Status: loading Pyodide (may take a few seconds)...";
  if (typeof loadPyodide !== "function") {
    const s = document.createElement("script");
    s.src = "https://cdn.jsdelivr.net/pyodide/v0.23.4/full/pyodide.js";
    document.head.appendChild(s);
    await new Promise(res => { s.onload = res; });
  }
  pyodide = await loadPyodide({ indexURL: "https://cdn.jsdelivr.net/pyodide/v0.23.4/full/" });
  statusEl.textContent = "Status: installing micropip & packages...";
  await pyodide.loadPackage(["micropip"]);
  const micropip = pyodide.pyimport("micropip");
  await micropip.install("pandas");
  await micropip.install("matplotlib");
  await pyodide.loadPackage(["numpy"]);
  statusEl.textContent = "Status: Python environment ready.";
  pyReady = true;
}

async function tryFetchCandidates(candidates) {
  for (const url of candidates) {
    statusEl.textContent = `Status: attempting to fetch ${url} ...`;
    try {
      const r = await fetch(url);
      if (!r.ok) {
        console.debug(`Candidate ${url} returned ${r.status}`);
        continue;
      }
      const txt = await r.text();
      statusEl.textContent = `Status: loaded ${url}`;
      return { url, text: txt };
    } catch (e) {
      console.debug(`Fetch ${url} failed:`, e);
      continue;
    }
  }
  throw new Error("All CSV fetch attempts failed.");
}

// ---------- Initialization ----------
(async () => {
  try {
    await loadPy();
  } catch (err) {
    console.error("Pyodide load failed:", err);
    statusEl.textContent = "Status: Pyodide load failed: " + String(err);
    return;
  }

  // fetch CSV text
  let csvResult = null;
  try {
    csvResult = await tryFetchCandidates(CSV_CANDIDATES);
  } catch (fetchErr) {
    console.error("CSV fetch attempts failed:", fetchErr);
    statusEl.textContent = "Status: failed to fetch chap_lemmas.csv. Place file under docs/data and/or update CSV_CANDIDATES.";
    return;
  }

  csvTextCache = csvResult.text || "";
  statusEl.textContent = `Status: building Python state from ${csvResult.url}...`;

  try {
    pyodide.globals.set("csv_text", csvTextCache);
    const ok = await pyodide.runPythonAsync(initPyGlobals);
    try { pyodide.globals.delete("csv_text"); } catch (e) {}
    if (ok !== "OK") {
      console.warn("Python init returned:", ok);
    }

    const py_df = pyodide.globals.get("chap_df");
    const py_counters = pyodide.globals.get("chapter_counters");

    if (!py_df || !py_counters) {
      console.error("Missing Python globals after init:", { py_df, py_counters });
      statusEl.textContent = "Status: Python init failed — chap_df or chapter_counters not set. See console.";
      return;
    }

    pyState = { df_proxy: py_df, counters_proxy: py_counters, source: csvResult.url };
    statusEl.textContent = `Status: ready (using ${csvResult.url}). Enter a lemmatized word and click Plot.`;
  } catch (pyErr) {
    console.error("Python initialization error:", pyErr);
    try { console.warn("CSV preview (first 800 chars):", (csvTextCache||"").slice(0,800)); } catch(e){}
    statusEl.textContent = "Status: Python initialization failed — see console for details.";
    return;
  }
})();

// ---------- Plotting function ----------
async function plotLemma(query, normalize) {
  if (!pyReady) {
    statusEl.textContent = "Status: Pyodide not ready.";
    return;
  }
  if (!pyState || !pyState.df_proxy || !pyState.counters_proxy) {
    statusEl.textContent = "Status: Python state not initialized. Try reloading the page.";
    console.error("plotLemma called but pyState is missing or malformed:", pyState);
    return;
  }

  statusEl.textContent = "Status: computing counts & plotting...";
  try {
    // inject handles expected by plot template
    pyodide.globals.set("df", pyState.df_proxy);
    pyodide.globals.set("counters", pyState.counters_proxy);
    pyodide.globals.set("query", query);
    pyodide.globals.set("normalize_flag", normalize);

    // run and receive tuple: (b64_png, out_json)
    const result = await pyodide.runPythonAsync(plotPyTemplate);

    // result behaves like an array-like proxy: index 0 -> b64 image, index 1 -> json string
    const b64 = result[0];
    const out_json = result[1];

    // cleanup injected globals
    try { pyodide.globals.delete("df"); } catch(e){}
    try { pyodide.globals.delete("counters"); } catch(e){}
    try { pyodide.globals.delete("query"); } catch(e){}
    try { pyodide.globals.delete("normalize_flag"); } catch(e){}

    // parse JSON safely
    let data = null;
    try {
      data = JSON.parse(out_json);
    } catch (e) {
      console.warn("Failed to parse out_json:", e, out_json);
      data = null;
    }

    // render image and accessible report
    plotEl.innerHTML = "";
    const img = document.createElement("img");
    img.src = "data:image/png;base64," + b64;
    img.alt = `Lemma frequency: ${query}`;
    img.style.maxWidth = "100%";
    img.tabIndex = 0; // keyboard focusable

    // accessible description region id
    const descId = "plot-desc-" + Math.random().toString(36).slice(2,9);
    img.setAttribute("aria-describedby", descId);

    plotEl.appendChild(img);

    // Create region that contains summary, CSV download, table
    const report = document.createElement("div");
    report.id = descId;
    report.style.marginTop = "10px";
    report.setAttribute("role", "region");
    report.setAttribute("aria-live", "polite");
    report.setAttribute("aria-label", `Numeric summary for ${query}`);
    plotEl.appendChild(report);

    if (data) {
      // trend sentence (from Python)
      const trendP = document.createElement("p");
      trendP.style.margin = "4px 0";
      trendP.textContent = data.trend_text || "";
      trendP.style.fontStyle = "italic";
      report.appendChild(trendP);

      // brief numeric summary
      const summaryP = document.createElement("p");
      summaryP.style.margin = "4px 0";
      const maxCh = (data.max_chapters || []).join(", ") || "N/A";
      const minCh = (data.min_chapters || []).join(", ") || "N/A";
      const total = (data.total_raw !== undefined) ? data.total_raw : "N/A";
      summaryP.textContent = `${data.n_chapters} chapters. Total occurrences (raw): ${total}. Highest raw count: ${data.max_raw} (chapter(s) ${maxCh}). Lowest raw count: ${data.min_raw} (chapter(s) ${minCh}).`;
      report.appendChild(summaryP);

      // CSV download
      const csvBtn = document.createElement("a");
      csvBtn.style.display = "inline-block";
      csvBtn.style.margin = "6px 0";
      csvBtn.textContent = "Download chapter counts (CSV)";
      csvBtn.setAttribute("role","button");
      csvBtn.setAttribute("aria-pressed","false");

      const csvLines = ["chapter,word_count,raw_count" + (data.counts_norm ? ",normalized_per_1k" : "")];
      for (let i=0;i<data.n_chapters;i++){
        const chap = i+1;
        const wc = (data.word_counts && data.word_counts[i]!==undefined) ? data.word_counts[i] : "";
        const raw = (data.counts_raw && data.counts_raw[i]!==undefined) ? data.counts_raw[i] : 0;
        const norm = (data.counts_norm && data.counts_norm[i]!==undefined) ? data.counts_norm[i] : "";
        csvLines.push([chap, wc, raw, norm].filter((_,idx)=> idx<3 || data.counts_norm).join(","));
      }
      const csvContent = csvLines.join("\n");
      const blob = new Blob([csvContent], {type:"text/csv"});
      const blobUrl = URL.createObjectURL(blob);
      csvBtn.href = blobUrl;
      csvBtn.download = `dracula_${query}_chapter_counts.csv`;
      report.appendChild(csvBtn);

      // Accessible table
      const table = document.createElement("table");
      table.style.width = "100%";
      table.style.borderCollapse = "collapse";
      table.style.marginTop = "8px";
      table.setAttribute("role", "table");
      table.setAttribute("aria-describedby", descId + "-caption");

      const caption = document.createElement("caption");
      caption.id = descId + "-caption";
      caption.style.textAlign = "left";
      caption.style.fontWeight = "600";
      caption.style.marginBottom = "4px";
      caption.textContent = `Chapter counts for lemma "${data.query}"`;
      table.appendChild(caption);

      const thead = document.createElement("thead");
      const hrow = document.createElement("tr");
      ["Chapter","Word count","Raw occurrences", normalize ? "Normalized per 1k words" : null].forEach(h=>{
        if (!h) return;
        const th = document.createElement("th");
        th.textContent = h;
        th.scope = "col";
        th.style.borderBottom = "1px solid #ccc";
        th.style.padding = "6px 4px";
        hrow.appendChild(th);
      });
      thead.appendChild(hrow);
      table.appendChild(thead);

      const tbody = document.createElement("tbody");
      for (let i=0;i<data.n_chapters;i++){
        const tr = document.createElement("tr");
        const chapCell = document.createElement("th");
        chapCell.scope = "row";
        chapCell.textContent = (i+1).toString();
        chapCell.style.padding = "6px 4px";
        tr.appendChild(chapCell);

        const wcCell = document.createElement("td");
        wcCell.textContent = (data.word_counts && data.word_counts[i]!==undefined) ? data.word_counts[i] : "";
        wcCell.style.padding = "6px 4px";
        tr.appendChild(wcCell);

        const rawCell = document.createElement("td");
        rawCell.textContent = (data.counts_raw && data.counts_raw[i]!==undefined) ? data.counts_raw[i] : 0;
        rawCell.style.padding = "6px 4px";
        tr.appendChild(rawCell);

        if (normalize) {
          const normCell = document.createElement("td");
          normCell.textContent = (data.counts_norm && data.counts_norm[i]!==undefined && data.counts_norm[i] !== null) ? Number(data.counts_norm[i]).toFixed(2) : "";
          normCell.style.padding = "6px 4px";
          tr.appendChild(normCell);
        }

        tbody.appendChild(tr);
      }
      table.appendChild(tbody);
      report.appendChild(table);
    } else {
      const p2 = document.createElement("p");
      p2.textContent = "No numeric data available for this query.";
      report.appendChild(p2);
    }

    statusEl.textContent = "Status: done.";
    // Optionally focus the image so keyboard / screen reader users find it quickly:
    // img.focus();

  } catch (err) {
    console.error("Error during plot:", err);
    let msg = String(err);
    statusEl.textContent = "Status: error during plotting — see console. " + (msg.split("\n")[0] || "");
    plotEl.innerHTML = "<pre style='color:red'>Error: " + (msg.split('\n')[0] || msg) + "</pre>";
  }
}

// ---------- UI wiring ----------
plotBtn.addEventListener("click", () => {
  const q = lemmaInput.value.trim();
  if (!q) { alert("Enter a lemmatized single word (e.g., 'vampire')."); return; }
  plotLemma(q, normalizeBox.checked);
});
lemmaInput.addEventListener("keydown", (e) => { if (e.key === "Enter") plotBtn.click(); });

</script>
