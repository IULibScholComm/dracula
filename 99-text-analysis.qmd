# Text analysis: Dracula

```{python}
#| label: imports_analysis
#| echo: false
#| eval: true
#| cache: true
#| freeze: auto
#| warning: false
#| error: false

from pathlib import Path
import re
import logging
import sys
import json
import pandas as pd
import numpy as np
import nltk
from collections import Counter
import time

# optional heavy deps (import when needed so Quarto render won't fail if absent)
# from nltk.sentiment.vader import SentimentIntensityAnalyzer
# import spacy
# from geopy.geocoders import Nominatim
# import folium
# import plotly.express as px
# from IPython.display import IFrame, display

# Logging config
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

# Ensure data dir
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
RESOURCES_DIR = Path("resources")
RESOURCES_DIR.mkdir(exist_ok=True)

```

```{python}
#| label: load_clean_witness
#| echo: false
#| eval: true
#| cache: true
#| freeze: auto
#| warning: false
#| error: false

def load_chapters_from_clean_text(path: Path):
    """
    Read a cleaned text file and split into chapters / preface.
    Accepts markers like:
      [PREFACE], [CHAPTER 1], [CHAPTER 01], [CHAPTER 10], [FULL_TEXT]
    Returns DataFrame with columns: chapter (int), marker (str), text (str)
    """
    txt = path.read_text(encoding="utf-8")
    # Accept any digits for CHAPTER
    parts = re.split(r'(?m)^\[(PREFACE|CHAPTER\s+\d+|FULL_TEXT)\]\s*$', txt, flags=re.IGNORECASE)
    chapters = []
    if len(parts) < 3:
        # fallback: naive splitting on lines that start with [CHAPTER
        logging.warning("Primary chapter-splitting pattern did not find markers; using fallback split.")
        split_points = re.split(r'(?m)^\[CHAPTER\s+\d+\]', txt)
        for i, block in enumerate(split_points, start=1):
            chapters.append({"chapter": i, "marker": f"CHAPTER {i:02d}", "text": block.strip()})
    else:
        i = 1
        for j in range(1, len(parts), 2):
            marker = parts[j].strip()
            body = parts[j+1].strip() if (j+1) < len(parts) else ""
            m = re.search(r'CHAPTER\s+(\d+)', marker, flags=re.IGNORECASE)
            if m:
                idx = int(m.group(1))
            elif marker.strip().upper().startswith("PREFACE"):
                idx = 0
            else:
                idx = i
            chapters.append({"chapter": idx, "marker": marker, "text": body})
            i += 1

    # sort by numeric chapter (preface=0 at top)
    chapters = sorted(chapters, key=lambda r: (r['chapter'] if isinstance(r['chapter'], int) else 9999))
    df = pd.DataFrame([{"chapter": int(c["chapter"]), "marker": c["marker"], "text": c["text"]} for c in chapters])
    logging.info(f"Loaded {len(df)} chapter blocks (including PREFACE if present).")
    return df

clean_path = RESOURCES_DIR / "dracula_clean.txt"
if not clean_path.exists():
    raise FileNotFoundError(f"{clean_path} not found. Place the cleaned text at {clean_path}")

chap_df = load_chapters_from_clean_text(clean_path)

```

```{python}
#| label: compute_or_load_chap_lemmas
#| echo: false
#| eval: true
#| cache: true
#| freeze: auto
#| warning: false
#| error: false

import importlib
from typing import Optional

CACHE_PATH = DATA_DIR / "chap_lemmas.csv"

def compute_lemmas_df(chap_df: pd.DataFrame, use_spacy: bool = True, spacy_model: str = "en_core_web_sm") -> pd.DataFrame:
    df = chap_df.copy().sort_values("chapter").reset_index(drop=True)
    df["text"] = df["text"].fillna("").astype(str)
    df["word_count"] = df["text"].str.split().str.len().fillna(0).astype(int)

    nlp = None
    if use_spacy:
        try:
            import spacy
            # try load model
            nlp = spacy.load(spacy_model, disable=["ner","parser","textcat"])
            logging.info(f"Loaded spaCy model: {spacy_model}")
        except Exception as e:
            logging.warning(f"spaCy model load failed ({e}); falling back to token-only processing.")
            nlp = None

    lemmas_list = []
    texts = df["text"].tolist()

    if nlp is not None:
        # prefer nlp.pipe with safe parameters; avoid n_process unless explicitly supported
        for doc in nlp.pipe(texts, batch_size=8):
            lemma_tokens = [tok.lemma_.lower() for tok in doc if tok.is_alpha]
            lemmas_list.append(" ".join(lemma_tokens))
    else:
        # simple fallback tokenization (alphabetic only)
        for t in texts:
            lemmas_list.append(" ".join(re.findall(r"[A-Za-z]+", t.lower())))

    df_out = pd.DataFrame({
        "chapter": df["chapter"].astype(int),
        "marker": df.get("marker", df["chapter"].astype(str)),
        "lemmas_str": lemmas_list,
        "word_count": df["word_count"].astype(int)
    })

    df_out.to_csv(CACHE_PATH, index=False, encoding="utf-8")
    logging.info(f"Wrote lemma cache to {CACHE_PATH} ({len(df_out)} rows).")
    return df_out

def load_or_compute_lemmas(chap_df: pd.DataFrame, force_recompute: bool = False) -> pd.DataFrame:
    if CACHE_PATH.exists() and not force_recompute:
        try:
            df_cached = pd.read_csv(CACHE_PATH, encoding="utf-8")
            df_cached["chapter"] = pd.to_numeric(df_cached["chapter"], errors="coerce").fillna(0).astype(int)
            df_cached["word_count"] = pd.to_numeric(df_cached.get("word_count", 0), errors="coerce").fillna(0).astype(int)
            logging.info(f"Loaded cached lemmas from {CACHE_PATH}")
            return df_cached
        except Exception as e:
            logging.warning(f"Failed to load cached lemmas ({e}), will recompute.")
    # compute using spaCy if available
    try:
        return compute_lemmas_df(chap_df, use_spacy=True)
    except Exception as e:
        logging.warning(f"spaCy computation failed: {e} — falling back to tokenization-only.")
        return compute_lemmas_df(chap_df, use_spacy=False)

# set to True if you want to force recompute in Quarto run (e.g., for reproducible build)
FORCE_RECOMPUTE_LEMMAS = False
chap_lemmas_df = load_or_compute_lemmas(chap_df, force_recompute=FORCE_RECOMPUTE_LEMMAS)

```

```{python}
#| label: widgets_termfreq_lemmatized
#| echo: false
#| eval: true
#| cache: true
#| warning: false
#| error: false

"""
Interactive lemmatized term-frequency widget for Binder/Jupyter.

Single-word queries only (lemmatized forms). For Quarto static renders we print
instructions explaining how to use the widget in Binder.
"""

import logging
import re
import pandas as pd
from collections import Counter

# Defensive checks
if "chap_lemmas_df" not in globals():
    raise RuntimeError("chap_lemmas_df not found — run the chapter/lemma cells before this one.")

# --------------------------------------------------
# Precompute per-chapter lemma counters (single-word only)
# --------------------------------------------------
_chapter_counters = []
_chapter_word_counts = chap_lemmas_df["word_count"].astype(int).tolist()

# Build list of lemma strings to keep ordering stable for plotting
_lemma_strings_list = chap_lemmas_df["lemmas_str"].fillna("").astype(str).tolist()

for s in _lemma_strings_list:
    if not s:
        _chapter_counters.append(Counter())
    else:
        _chapter_counters.append(Counter(s.split()))

# Helper functions
def lemmatize_query_simple(q: str):
    q = q.strip().lower()
    if not q:
        return []
    return re.findall(r"[a-z]+", q)

def count_single_lemma(word: str):
    """
    Count a single lemmatized word per chapter.
    Extremely fast: Counter lookup per chapter.
    """
    w = word.strip().lower()
    if not w:
        return [0] * len(_chapter_counters)
    return [c.get(w, 0) for c in _chapter_counters]

def make_plot_df(lemmas_str_df: pd.DataFrame, query: str, normalize: bool = False):
    lemmas = lemmatize_query_simple(query)
    if len(lemmas) != 1:
        # enforce single-word queries — return empty DataFrame to signal nothing to plot
        return pd.DataFrame()

    counts = count_single_lemma(lemmas[0])

    plot_df = pd.DataFrame({
        "chapter": lemmas_str_df["chapter"].astype(int).tolist(),
        "count": counts,
        "marker": lemmas_str_df.get("marker", lemmas_str_df["chapter"].astype(str)).tolist(),
        "word_count": lemmas_str_df["word_count"].astype(int).tolist()
    }).sort_values("chapter").reset_index(drop=True)

    plot_df = plot_df[plot_df["chapter"] > 0].copy()  # exclude preface / aggregate (chapter==0)

    if plot_df.empty:
        return plot_df  # empty DataFrame to signal nothing to plot

    if normalize:
        plot_df["norm_per_1k"] = plot_df["count"] / (plot_df["word_count"].replace({0: 1}) / 1000.0)
    return plot_df

# Try to import interactive dependencies
INTERACTIVE = False
try:
    import ipywidgets as widgets
    from IPython.display import display, HTML, clear_output
    import plotly.express as px
    INTERACTIVE = True
except Exception:
    INTERACTIVE = False

if not INTERACTIVE:
    # Non-interactive build: show brief instructions for Binder usage
    print("This page includes an interactive term-frequency widget. To use it, launch this Quarto project in Binder (see project README).")
    print("In Binder's Jupyter interface, open this notebook and type a single lemmatized word (e.g., 'vampire'), then click Plot.")
else:
    # Build UI: text input + normalize checkbox + plot area + helpful instructions
    term_input = widgets.Text(
        value="",
        placeholder="Type a single word (e.g. vampire)",
        description="Term:",
        layout=widgets.Layout(width="60%")
    )

    normalize_chk = widgets.Checkbox(
        value=False,
        description="Normalize (per 1k words)",
        indent=False
    )

    plot_btn = widgets.Button(description="Plot", button_style="primary")
    out = widgets.Output(layout=widgets.Layout(width="100%"))

    def render_plot_for_query(query_text, normalize=False):
        """
        Build and show a plotly figure for the given single-word query_text.
        Returns True if successful, False if nothing to show.
        """
        q = query_text.strip()
        if not q:
            with out:
                clear_output(wait=True)
                print("Please enter a single lemmatized word to plot.")
            return False

        lemmas = lemmatize_query_simple(q)
        if len(lemmas) != 1:
            with out:
                clear_output(wait=True)
                print("Please enter a single word (phrases are not supported).")
            return False

        plot_df = make_plot_df(chap_lemmas_df, q, normalize=normalize)
        with out:
            clear_output(wait=True)
            if plot_df.empty:
                print(f"No occurrences of '{q}' found in chapter texts.")
                return False

            # Pick column to plot
            ycol = "norm_per_1k" if normalize and "norm_per_1k" in plot_df.columns else "count"
            ylabel = "Occurrences per 1,000 words" if ycol == "norm_per_1k" else "Raw occurrences"

            try:
                fig = px.line(
                    plot_df,
                    x="chapter",
                    y=ycol,
                    markers=True,
                    title=f"Frequency of '{q}' by chapter",
                    labels={"chapter": "Chapter", ycol: ylabel},
                    hover_data=["marker", "chapter", ycol]
                )
                fig.update_traces(mode="lines+markers")
                fig.update_layout(height=420, margin=dict(l=40, r=20, t=60, b=40), xaxis=dict(dtick=1))
                fig.show()
                return True
            except Exception as e:
                print("Plotly failed to render the figure:", e)
                return False

    # Wire events: button click + Enter key on input triggers the same behavior
    def on_click_btn(b):
        render_plot_for_query(term_input.value, normalize_chk.value)

    plot_btn.on_click(on_click_btn)

    # bind Enter key if Text has on_submit (ipykernel >= ?)
    try:
        term_input.on_submit(lambda widget: render_plot_for_query(widget.value, normalize_chk.value))
    except Exception:
        # fallback: helpful tip in UI; user can click Plot
        pass

    # render the UI
    display(HTML("<h4>Search term frequency across chapters (lemmatized)</h4>"))
    display(widgets.HBox([term_input, plot_btn, normalize_chk]))
    display(out)

    # Optionally show a short hint / example
    with out:
        print("Type a word (e.g., 'vampire') and click Plot. Press Enter in the textbox where supported.")

```

---
format: html
execute:
  echo: false
  enabled: true
---

<div style="max-width:900px">
  <p>Enter a single **lemmatized** word (no multi-word phrases). Example: <code>vampire</code></p>

  <label for="lemma">Lemma:</label>
  <input id="lemma" placeholder="vampire" style="width:220px" />
  <button id="plotBtn">Plot</button>
  <label style="margin-left:12px"><input type="checkbox" id="normalize" /> Normalize (per 1k words)</label>

  <!-- status is an ARIA live region -->
  <div id="status" style="margin-top:8px;color:#666" aria-live="polite" role="status">Status: initializing...</div>
  <div id="plotArea" style="margin-top:12px"></div>

  <p style="color:#666; margin-top:14px; font-size:0.95em">
    This lightweight page parses a precomputed CSV in-browser and produces an accessible plot + numeric table. If you need spaCy or NER, run the original notebook (Binder/Colab).
  </p>
</div>

<script type="module">
// ---------- Configuration ----------
const CSV_CANDIDATES = [
  "https://iulibscholcomm.github.io/dracula/data/chap_lemmas.csv",
  "/dracula/data/chap_lemmas.csv",
  "/data/chap_lemmas.csv",
  "data/chap_lemmas.csv",
  "chap_lemmas.csv"
];

const statusEl = document.getElementById("status");
const plotEl = document.getElementById("plotArea");
const plotBtn = document.getElementById("plotBtn");
const lemmaInput = document.getElementById("lemma");
const normalizeBox = document.getElementById("normalize");

let cachedChapters = null;

// ---------- Fetch & parse CSV ----------
async function fetchCsvCandidates(candidates) {
  for (const url of candidates) {
    statusEl.textContent = `Status: attempting to fetch ${url} ...`;
    try {
      const r = await fetch(url);
      if (!r.ok) continue;
      const text = await r.text();
      statusEl.textContent = `Status: loaded ${url}`;
      return { url, text };
    } catch (e) {
      console.debug("fetch failed", url, e);
      continue;
    }
  }
  throw new Error("All CSV fetch attempts failed.");
}

// Minimal CSV parser with quoted-field support
function parseCSV(text) {
  const rows = [];
  let cur = "", inQuotes = false, row = [];
  for (let i = 0; i < text.length; i++) {
    const ch = text[i];
    const nch = text[i+1];
    if (ch === '"') {
      if (inQuotes && nch === '"') { cur += '"'; i++; } // escaped quote
      else inQuotes = !inQuotes;
    } else if (ch === ',' && !inQuotes) {
      row.push(cur); cur = "";
    } else if ((ch === '\n' || ch === '\r') && !inQuotes) {
      if (ch === '\r' && nch === '\n') { i++; }
      row.push(cur);
      rows.push(row);
      row = [];
      cur = "";
    } else {
      cur += ch;
    }
  }
  if (cur !== "" || row.length > 0) { row.push(cur); rows.push(row); }
  return rows;
}

function csvRowsToObjects(rows) {
  if (!rows || rows.length === 0) return [];
  const header = rows[0].map(h => h.trim());
  const objs = [];
  for (let i = 1; i < rows.length; i++) {
    if (rows[i].length === 1 && rows[i][0].trim() === "") continue;
    const obj = {};
    for (let j = 0; j < header.length; j++) {
      obj[header[j]] = rows[i][j] !== undefined ? rows[i][j] : "";
    }
    objs.push(obj);
  }
  return objs;
}

// ---------- Data prep ----------
function buildChapterCounters(objs) {
  return objs.map(o => {
    const chap = Number(o.chapter) || 0;
    const lemmas = (o.lemmas_str || "").trim();
    const word_count = Number(o.word_count) || 0;
    const tokens = lemmas ? lemmas.split(/\s+/) : [];
    const freq = {};
    for (const t of tokens) {
      const key = t.toLowerCase();
      freq[key] = (freq[key] || 0) + 1;
    }
    return { chapter: chap, freq, word_count };
  });
}

function computeCounts(chapters, query, normalizeFlag) {
  const q = query.trim().toLowerCase();
  const raw = chapters.map(c => c.freq[q] || 0);
  const words = chapters.map(c => c.word_count || 0);
  const norm = normalizeFlag ? raw.map((v,i) => {
    const wc = words[i] || 1;
    return (v / wc) * 1000.0;
  }) : null;
  return { raw, norm, words };
}

// ---------- Trend calc ----------
function computeTrend(series) {
  const n = series.length;
  if (n < 2) return { text: "Not enough points to compute trend.", slope: 0, r: 0 };
  const x = Array.from({length:n}, (_,i)=>i+1);
  const mean = arr => arr.reduce((a,b)=>a+b,0)/arr.length;
  const mx = mean(x), my = mean(series);
  let num = 0, denx = 0, deny = 0;
  for (let i=0;i<n;i++){
    num += (x[i]-mx)*(series[i]-my);
    denx += (x[i]-mx)*(x[i]-mx);
    deny += (series[i]-my)*(series[i]-my);
  }
  const slope = denx === 0 ? 0 : num/denx;
  const r = (denx*deny === 0) ? 0 : (num / Math.sqrt(denx*deny));
  let text = "";
  if (Math.abs(r) >= 0.4) {
    text = slope > 0 ? `Increasing trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`
                     : `Decreasing trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`;
  } else if (Math.abs(r) >= 0.2) {
    text = slope > 0 ? `Mild upward trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`
                     : `Mild downward trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`;
  } else {
    text = `No clear trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`;
  }
  return { text, slope, r };
}

// ---------- SVG plot renderer ----------
function renderSvgPlot(values, opts={width:900,height:260, stroke:'#1b4f72'}) {
  const W = opts.width, H = opts.height;
  const pad = {left:40,right:12,top:18,bottom:36};
  const n = Math.max(values.length, 1);
  const maxv = Math.max(...values, 1);
  const minv = Math.min(...values, 0);
  const span = (maxv - minv) || 1;
  const x = i => pad.left + ( (n===1) ? (W - pad.left - pad.right)/2 : (i/(n-1))*(W - pad.left - pad.right) );
  const y = v => pad.top + (1 - (v - minv)/span)*(H - pad.top - pad.bottom);

  const pts = values.map((v,i)=>`${x(i)},${y(v)}`).join(' ');

  const svg = document.createElementNS("http://www.w3.org/2000/svg","svg");
  svg.setAttribute("viewBox", `0 0 ${W} ${H}`);
  svg.setAttribute("width", "100%");
  svg.setAttribute("height", H);
  svg.setAttribute("role","img");
  svg.style.maxWidth = "100%";
  svg.style.outline = "none";

  const ny = 4;
  for (let i=0;i<=ny;i++){
    const val = minv + (i/ny)*span;
    const yPos = y(val);
    const line = document.createElementNS(svg.namespaceURI,'line');
    line.setAttribute('x1', pad.left);
    line.setAttribute('x2', W - pad.right);
    line.setAttribute('y1', yPos);
    line.setAttribute('y2', yPos);
    line.setAttribute('stroke', '#eee');
    line.setAttribute('stroke-width','1');
    svg.appendChild(line);

    const tx = document.createElementNS(svg.namespaceURI,'text');
    tx.setAttribute('x', 8);
    tx.setAttribute('y', yPos - 2);
    tx.setAttribute('font-size','11');
    tx.setAttribute('fill','#333');
    tx.textContent = Number(val).toFixed(0);
    svg.appendChild(tx);
  }

  const pl = document.createElementNS(svg.namespaceURI,'polyline');
  pl.setAttribute('points', pts);
  pl.setAttribute('fill','none');
  pl.setAttribute('stroke', opts.stroke || '#000');
  pl.setAttribute('stroke-width','2');
  svg.appendChild(pl);

  for (let i=0;i<values.length;i++){
    const c = document.createElementNS(svg.namespaceURI,'circle');
    c.setAttribute('cx', x(i));
    c.setAttribute('cy', y(values[i]));
    c.setAttribute('r','3');
    c.setAttribute('fill', opts.stroke || '#000');
    svg.appendChild(c);
  }

  for (let i=0;i<values.length;i++){
    if (values.length > 30 && (i % Math.ceil(values.length/20) !== 0)) continue;
    const tx = document.createElementNS(svg.namespaceURI,'text');
    tx.setAttribute('x', x(i));
    tx.setAttribute('y', H - 8);
    tx.setAttribute('font-size','10');
    tx.setAttribute('text-anchor','middle');
    tx.setAttribute('fill','#333');
    tx.textContent = (i+1).toString();
    svg.appendChild(tx);
  }

  return svg;
}

// ---------- Accessible report builder ----------
function buildAccessibleReport(container, query, countsRaw, countsNorm, wordCounts, normalizeFlag, trendText) {
  container.innerHTML = "";

  const trendP = document.createElement("p");
  trendP.style.fontStyle = "italic";
  trendP.textContent = trendText || "";
  container.appendChild(trendP);

  const total = countsRaw.reduce((a,b)=>a+b,0);
  const maxv = Math.max(...countsRaw);
  const maxCh = countsRaw.map((v,i)=>v===maxv?(i+1):null).filter(Boolean).join(", ") || "N/A";
  const minv = Math.min(...countsRaw);
  const minCh = countsRaw.map((v,i)=>v===minv?(i+1):null).filter(Boolean).join(", ") || "N/A";
  const summaryP = document.createElement("p");
  summaryP.textContent = `${countsRaw.length} chapters. Total occurrences (raw): ${total}. Highest raw count: ${maxv} (chapter(s) ${maxCh}). Lowest raw count: ${minv} (chapter(s) ${minCh}).`;
  container.appendChild(summaryP);

  const csvLines = ["chapter,word_count,raw_count" + (countsNorm ? ",normalized_per_1k" : "")];
  for (let i=0;i<countsRaw.length;i++){
    const chap = i+1;
    const wc = wordCounts[i] || "";
    const raw = countsRaw[i] || 0;
    const norm = countsNorm ? countsNorm[i] : "";
    csvLines.push([chap, wc, raw, norm].filter((_,idx)=> idx<3 || countsNorm).join(","));
  }
  const csvBlob = new Blob([csvLines.join("\n")], {type: "text/csv"});
  const url = URL.createObjectURL(csvBlob);
  const a = document.createElement("a");
  a.href = url;
  a.download = `dracula_${query}_chapter_counts.csv`;
  a.textContent = "Download chapter counts (CSV)";
  a.style.display = "inline-block";
  a.style.margin = "6px 0";
  container.appendChild(a);

  const table = document.createElement("table");
  table.style.width = "100%";
  table.style.borderCollapse = "collapse";
  table.style.marginTop = "8px";
  table.setAttribute("role","table");

  const caption = document.createElement("caption");
  caption.textContent = `Chapter counts for lemma "${query}"`;
  caption.style.textAlign = "left";
  caption.style.fontWeight = "600";
  table.appendChild(caption);

  const thead = document.createElement("thead");
  const thr = document.createElement("tr");
  ["Chapter","Word count","Raw occurrences", normalizeFlag ? "Normalized per 1k words" : null].forEach(h=>{
    if (!h) return;
    const th = document.createElement("th");
    th.scope = "col";
    th.textContent = h;
    th.style.borderBottom = "1px solid #ccc";
    th.style.padding = "6px 4px";
    thr.appendChild(th);
  });
  thead.appendChild(thr);
  table.appendChild(thead);

  const tbody = document.createElement("tbody");
  for (let i=0;i<countsRaw.length;i++){
    const tr = document.createElement("tr");
    const th = document.createElement("th");
    th.scope = "row";
    th.textContent = (i+1).toString();
    th.style.padding = "6px 4px";
    tr.appendChild(th);

    const tdW = document.createElement("td");
    tdW.textContent = wordCounts[i] || "";
    tdW.style.padding = "6px 4px";
    tr.appendChild(tdW);

    const tdR = document.createElement("td");
    tdR.textContent = countsRaw[i] || 0;
    tdR.style.padding = "6px 4px";
    tr.appendChild(tdR);

    if (normalizeFlag) {
      const tdN = document.createElement("td");
      tdN.textContent = (countsNorm && countsNorm[i]!==undefined && countsNorm[i]!==null) ? Number(countsNorm[i]).toFixed(2) : "";
      tdN.style.padding = "6px 4px";
      tr.appendChild(tdN);
    }

    tbody.appendChild(tr);
  }
  table.appendChild(tbody);
  container.appendChild(table);
}

// ---------- Main UI flow ----------
async function initializeData() {
  try {
    const res = await fetchCsvCandidates(CSV_CANDIDATES);
    const rows = parseCSV(res.text);
    const objs = csvRowsToObjects(rows);

    if (objs.length === 0 || !('lemmas_str' in objs[0])) {
      statusEl.textContent = "Status: CSV missing expected columns (chapter, lemmas_str, word_count).";
      throw new Error("CSV parse/format issue");
    }

    cachedChapters = buildChapterCounters(objs);
    statusEl.textContent = `Status: ready (using ${res.url}). Enter a lemmatized word and click Plot.`;
  } catch (e) {
    console.error("Data init error:", e);
    statusEl.textContent = "Status: failed to load or parse chap_lemmas.csv. Check console.";
  }
}

async function plotLemma(query, normalizeFlag) {
  if (!cachedChapters) {
    statusEl.textContent = "Status: data not initialized.";
    return;
  }
  if (!query || !query.trim()) { alert("Enter a lemmatized single word (e.g., 'vampire')."); return; }

  statusEl.textContent = "Status: computing counts & plotting...";
  try {
    const { raw, norm, words } = computeCounts(cachedChapters, query, normalizeFlag);
    const seriesForTrend = normalizeFlag ? norm : raw;
    const trend = computeTrend(seriesForTrend.map(v => Number(v || 0)));

    plotEl.innerHTML = "";
    const heading = document.createElement("div");
    heading.textContent = `Frequency of '${query}' (lemmatized) — n_chapters=${raw.length}`;
    heading.style.fontWeight = "600";
    heading.style.marginBottom = "6px";
    plotEl.appendChild(heading);

    const svgValues = (normalizeFlag && norm) ? norm : raw;
    const svg = renderSvgPlot(svgValues, {width:900,height:260, stroke:'#1b4f72'});
    svg.setAttribute("tabindex", "0");
    const desc = document.createElement("div");
    desc.style.marginTop = "8px";
    desc.setAttribute("role","region");
    desc.setAttribute("aria-live","polite");
    desc.setAttribute("id", "desc-" + Math.random().toString(36).slice(2,8));
    svg.setAttribute("aria-describedby", desc.id);
    plotEl.appendChild(svg);
    plotEl.appendChild(desc);

    buildAccessibleReport(desc, query, raw, norm, words, normalizeFlag, trend.text);

    statusEl.textContent = "Status: done.";
  } catch (err) {
    console.error("Plot error:", err);
    statusEl.textContent = "Status: error during plotting — see console.";
    plotEl.innerHTML = `<pre style="color:red">Error: ${String(err)}</pre>`;
  }
}

// UI wiring
plotBtn.addEventListener("click", () => plotLemma(lemmaInput.value.trim(), normalizeBox.checked));
lemmaInput.addEventListener("keydown", (e) => { if (e.key === "Enter") plotBtn.click(); });

// init
initializeData();
</script>




```{python}
#| label: ner_spacy_per_chapter
#| echo: false
#| eval: false
#| cache: true
#| freeze: auto
#| warning: false
#| error: false

try:
    import spacy
    nlp = spacy.load("en_core_web_sm")
    logging.info("spaCy model loaded for NER.")
except Exception as e:
    logging.warning(f"spaCy model not available ({e}). Skipping NER extraction and writing empty CSVs.")
    nlp = None

OUTPUT_PLACES_BY_CH = DATA_DIR / "places_by_chapter_raw.csv"
OUTPUT_PLACE_COUNTS = DATA_DIR / "place_counts_raw.csv"
OUTPUT_PLACES_PER_CH = DATA_DIR / "places_per_chapter_raw.csv"
OUTPUT_PLACES_JSON = DATA_DIR / "places_per_chapter.json"

rows = []
if nlp:
    # conservative heuristics
    DIRECTIONAL = set(["north","south","east","west","northern","southern","eastern","western","left","right","up","down","here","there"])
    SHORT_MIN_CHARS = 3
    _initials_re = re.compile(r'^(?:[A-Z]\.){1,3}$')
    _digits_re = re.compile(r'\d')

    for _, row in chap_df.iterrows():
        chap = row['chapter']
        text = row['text'] or ""
        doc = nlp(text)
        for ent in doc.ents:
            if ent.label_ not in ("GPE","LOC","FAC"):
                continue
            raw = ent.text.strip()
            candidate = re.sub(r'\s+', ' ', raw).strip(" \t\n\r,.;:()[]\"'")
            low = candidate.lower()
            if len(candidate) < SHORT_MIN_CHARS:
                continue
            if _initials_re.match(candidate):
                continue
            if _digits_re.search(candidate):
                continue
            if low in DIRECTIONAL:
                continue
            if low in {"and","the","that","this","there","here","it","i","we","you","he","she","they"}:
                continue
            # do not over-normalize proper names: keep as-is but strip extra spaces
            rows.append({"place": candidate, "chapter": int(chap) if not pd.isna(chap) else 0})

# write outputs (even if empty)
places_by_ch = pd.DataFrame(rows, columns=["place","chapter"])
places_by_ch.to_csv(OUTPUT_PLACES_BY_CH, index=False, encoding="utf-8")
if not places_by_ch.empty:
    place_counts = places_by_ch['place'].value_counts().reset_index()
    place_counts.columns = ['place','count']
    place_counts.to_csv(OUTPUT_PLACE_COUNTS, index=False, encoding="utf-8")

    places_per_ch = places_by_ch.groupby(['place','chapter'], dropna=False).size().reset_index(name='count')
    places_per_ch.to_csv(OUTPUT_PLACES_PER_CH, index=False, encoding="utf-8")
    places_per_ch.to_json(OUTPUT_PLACES_JSON, orient="records", force_ascii=False)
else:
    pd.DataFrame(columns=["place","count"]).to_csv(OUTPUT_PLACE_COUNTS, index=False, encoding="utf-8")
    pd.DataFrame(columns=["place","chapter","count"]).to_csv(OUTPUT_PLACES_PER_CH, index=False, encoding="utf-8")

logging.info("NER extraction complete — wrote CSVs (or empty skeletons).")
```

```{python}
#| label: prepare_geodata_for_plotly
#| echo: false
#| eval: false
#| cache: true
#| freeze: auto
#| warning: false
#| error: false

GEOCODE_CSV = DATA_DIR / "places_geocoded.csv"         # produced by a separate geocoding step
PLACES_PER_CH_RAW = DATA_DIR / "places_per_chapter_raw.csv"
OUT_CSV = DATA_DIR / "places_per_chapter.csv"
SUSPECT_CSV = DATA_DIR / "geocode_suspect_manual_review_needed.csv"

if not GEOCODE_CSV.exists():
    logging.warning(f"{GEOCODE_CSV} not found. Geocoding step must run separately. Skipping geodata preparation.")
else:
    geocoded = pd.read_csv(GEOCODE_CSV)
    if not PLACES_PER_CH_RAW.exists():
        raise FileNotFoundError("Run ner_spacy_per_chapter to produce places_per_chapter_raw.csv before preparing geodata.")
    places_per_ch = pd.read_csv(PLACES_PER_CH_RAW)

    geo_counts = places_per_ch.merge(geocoded, on="place", how="left")
    missing_coords = geo_counts[geo_counts['lat'].isna() | geo_counts['lon'].isna()]
    if not missing_coords.empty:
        logging.warning(f"{len(missing_coords)} place x chapter rows missing coordinates. See data/geocode_missing_coords.csv")
        missing_coords.to_csv(DATA_DIR / "geocode_missing_coords.csv", index=False, encoding="utf-8")

    suspect_rows = []
    euro_keywords = {"Transylvania","Bistritz","Bistrita","Romania","Bukovina","Klausenburgh","Borgo"}
    for _, r in geo_counts.iterrows():
        place = r.get("place")
        lat = r.get("lat")
        lon = r.get("lon")
        if pd.isna(lat) or pd.isna(lon):
            continue
        if any(k.lower() in str(place).lower() for k in euro_keywords):
            try:
                if not (-30 <= float(lon) <= 60):
                    suspect_rows.append({**r, "reason":"euro_name_geocoded_outside_europe"})
            except Exception:
                continue
    if suspect_rows:
        pd.DataFrame(suspect_rows).to_csv(SUSPECT_CSV, index=False, encoding="utf-8")
        logging.info(f"Wrote {len(suspect_rows)} suspect geocodes to {SUSPECT_CSV}")

    geo_counts = geo_counts.dropna(subset=["lat","lon"]).copy()
    geo_counts['lat'] = pd.to_numeric(geo_counts['lat'], errors='coerce')
    geo_counts['lon'] = pd.to_numeric(geo_counts['lon'], errors='coerce')
    geo_counts['count'] = pd.to_numeric(geo_counts['count'], errors='coerce').fillna(0).astype(int)
    geo_counts['chapter'] = pd.to_numeric(geo_counts['chapter'], errors='coerce').fillna(0).astype(int)

    agg_all = geo_counts.groupby(['place','lat','lon'], as_index=False)['count'].sum()
    agg_all['chapter'] = 0

    final = pd.concat([agg_all, geo_counts], ignore_index=True, sort=False)
    final = final[['place','lat','lon','chapter','count']].sort_values(['chapter','count'], ascending=[True,False])
    final.to_csv(OUT_CSV, index=False, encoding="utf-8")
    logging.info(f"Wrote merged per-chapter geodata to {OUT_CSV} ({len(final)} rows).")

```
```{python}
#| label: plotly_bubble_map
#| echo: true
#| eval: true
#| cache: true
#| freeze: auto
#| warning: false
#| error: false
#| code-fold: true

DATA_CSV = DATA_DIR / "places_per_chapter.csv"
if not DATA_CSV.exists():
    logging.warning("places_per_chapter.csv not present; run prepare_geodata_for_plotly first.")
else:
    import plotly.express as px

    df = pd.read_csv(DATA_CSV)
    df = df.dropna(subset=['lat','lon'])
    df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(int)
    df['chapter'] = pd.to_numeric(df['chapter'], errors='coerce').fillna(0).astype(int)

    # compute numeric chapter order for animation and a display label
    chapter_order = sorted(df['chapter'].unique().tolist())
    df['chapter_label'] = df['chapter'].apply(lambda c: "All chapters" if c == 0 else f"Chapter {int(c)}")

    max_count = df['count'].max() if not df.empty else 1
    desired_max_size = 60
    sizeref = 2.0 * max_count / (desired_max_size ** 2) if max_count > 0 else 1

    fig = px.scatter_geo(df,
                         lat='lat', lon='lon',
                         size='count',
                         hover_name='place',
                         hover_data={'count': True, 'chapter': True, 'chapter_label': True},
                         animation_frame='chapter',           # numeric ensures correct ordering
                         category_orders={'chapter': chapter_order},
                         projection='natural earth',
                         title="Geographic distribution of place references in Dracula (bubble size = mentions)")
    # set explicit sizeref on traces/frames
    if hasattr(fig, "frames") and fig.frames:
        for frame in fig.frames:
            for trace in frame.data:
                try:
                    trace.marker.sizeref = sizeref
                except Exception:
                    pass
    for trace in fig.data:
        try:
            trace.marker.sizeref = sizeref
        except Exception:
            pass

    fig.update_layout(height=600, margin=dict(l=10, r=10, t=60, b=10), template="plotly_white")
    # control animation speed gracefully (if controls exist)
    try:
        fig.layout.updatemenus[0].buttons[0].args[1]['frame']['duration'] = 800
        fig.layout.updatemenus[0].buttons[0].args[1]['transition']['duration'] = 300
    except Exception:
        pass

    fig.show()

```


```{python}
#| label: sentiment_vader
#| echo: false
#| eval: true
#| cache: true
#| freeze: auto
#| warning: false
#| error: false

# ensure VADER resource available; try to download quietly if missing
try:
    nltk.data.find("sentiment/vader_lexicon.zip")
except LookupError:
    logging.info("Downloading VADER lexicon (nltk).")
    nltk.download("vader_lexicon", quiet=True)

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

# compute raw VADER compound score per chapter (works for full chapter text)
chap_df = chap_df.copy()
chap_df['vader_compound'] = chap_df['text'].apply(lambda t: sid.polarity_scores(t)['compound'] if isinstance(t, str) else 0.0)

# compute word counts and guard against zero
chap_df['word_count'] = chap_df['text'].str.split().str.len().fillna(0).astype(int)
wc = chap_df['word_count'].replace({0: 1})   # safe divisor
chap_df['sentiment_per_1k_words'] = chap_df['vader_compound'] / (wc / 1000.0)

# persist for later use or external analysis
( DATA_DIR / "sentiment_by_chapter.csv" ).write_text(chap_df[['chapter','marker','word_count','vader_compound','sentiment_per_1k_words']].to_csv(index=False, encoding="utf-8"), encoding="utf-8")
logging.info("Wrote data/sentiment_by_chapter.csv")
```
```{python}
#| label: sentiment_line_plot_normalized
#| echo: true
#| eval: true
#| cache: true
#| freeze: auto
#| warning: false
#| error: false
#| code-fold: true

import plotly.express as px

plot_df = chap_df[chap_df["chapter"] > 0].sort_values("chapter").copy()
if plot_df.empty:
    print("No chapter data available to plot.")
else:
    plot_df["chapter"] = pd.to_numeric(plot_df["chapter"], errors="coerce")

    fig = px.line(plot_df, x="chapter", y="sentiment_per_1k_words", markers=True,
                  title="Length-normalized sentiment trajectory across Dracula",
                  labels={"chapter":"Chapter", "sentiment_per_1k_words":"Sentiment per 1,000 words (VADER)"},
                  hover_data={"marker":True, "sentiment_per_1k_words":":.3f"})
    fig.add_hline(y=0, line_dash="dash", line_color="gray", annotation_text="Neutral (0)", annotation_position="bottom right")
    fig.update_layout(margin=dict(l=60, r=20, t=60, b=60), height=420, xaxis=dict(dtick=1, tickmode="linear"), template="plotly_white")
    fig.show()

```