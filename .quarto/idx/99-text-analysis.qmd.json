{"title":"Text analysis: Dracula","markdown":{"headingText":"Text analysis: Dracula","containsRefs":false,"markdown":"\n```{python}\n#| label: imports_analysis\n#| echo: false\n#| eval: true\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n\nfrom pathlib import Path\nimport re\nimport logging\nimport sys\nimport json\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom collections import Counter\nimport time\n\n# optional heavy deps (import when needed so Quarto render won't fail if absent)\n# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n# import spacy\n# from geopy.geocoders import Nominatim\n# import folium\n# import plotly.express as px\n# from IPython.display import IFrame, display\n\n# Logging config\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n\n# Ensure data dir\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(exist_ok=True)\nRESOURCES_DIR = Path(\"resources\")\nRESOURCES_DIR.mkdir(exist_ok=True)\n\n```\n\n```{python}\n#| label: load_clean_witness\n#| echo: false\n#| eval: true\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n\ndef load_chapters_from_clean_text(path: Path):\n    \"\"\"\n    Read a cleaned text file and split into chapters / preface.\n    Accepts markers like:\n      [PREFACE], [CHAPTER 1], [CHAPTER 01], [CHAPTER 10], [FULL_TEXT]\n    Returns DataFrame with columns: chapter (int), marker (str), text (str)\n    \"\"\"\n    txt = path.read_text(encoding=\"utf-8\")\n    # Accept any digits for CHAPTER\n    parts = re.split(r'(?m)^\\[(PREFACE|CHAPTER\\s+\\d+|FULL_TEXT)\\]\\s*$', txt, flags=re.IGNORECASE)\n    chapters = []\n    if len(parts) < 3:\n        # fallback: naive splitting on lines that start with [CHAPTER\n        logging.warning(\"Primary chapter-splitting pattern did not find markers; using fallback split.\")\n        split_points = re.split(r'(?m)^\\[CHAPTER\\s+\\d+\\]', txt)\n        for i, block in enumerate(split_points, start=1):\n            chapters.append({\"chapter\": i, \"marker\": f\"CHAPTER {i:02d}\", \"text\": block.strip()})\n    else:\n        i = 1\n        for j in range(1, len(parts), 2):\n            marker = parts[j].strip()\n            body = parts[j+1].strip() if (j+1) < len(parts) else \"\"\n            m = re.search(r'CHAPTER\\s+(\\d+)', marker, flags=re.IGNORECASE)\n            if m:\n                idx = int(m.group(1))\n            elif marker.strip().upper().startswith(\"PREFACE\"):\n                idx = 0\n            else:\n                idx = i\n            chapters.append({\"chapter\": idx, \"marker\": marker, \"text\": body})\n            i += 1\n\n    # sort by numeric chapter (preface=0 at top)\n    chapters = sorted(chapters, key=lambda r: (r['chapter'] if isinstance(r['chapter'], int) else 9999))\n    df = pd.DataFrame([{\"chapter\": int(c[\"chapter\"]), \"marker\": c[\"marker\"], \"text\": c[\"text\"]} for c in chapters])\n    logging.info(f\"Loaded {len(df)} chapter blocks (including PREFACE if present).\")\n    return df\n\nclean_path = RESOURCES_DIR / \"dracula_clean.txt\"\nif not clean_path.exists():\n    raise FileNotFoundError(f\"{clean_path} not found. Place the cleaned text at {clean_path}\")\n\nchap_df = load_chapters_from_clean_text(clean_path)\n\n```\n\n```{python}\n#| label: compute_or_load_chap_lemmas\n#| echo: false\n#| eval: true\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n\nimport importlib\nfrom typing import Optional\n\nCACHE_PATH = DATA_DIR / \"chap_lemmas.csv\"\n\ndef compute_lemmas_df(chap_df: pd.DataFrame, use_spacy: bool = True, spacy_model: str = \"en_core_web_sm\") -> pd.DataFrame:\n    df = chap_df.copy().sort_values(\"chapter\").reset_index(drop=True)\n    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n    df[\"word_count\"] = df[\"text\"].str.split().str.len().fillna(0).astype(int)\n\n    nlp = None\n    if use_spacy:\n        try:\n            import spacy\n            # try load model\n            nlp = spacy.load(spacy_model, disable=[\"ner\",\"parser\",\"textcat\"])\n            logging.info(f\"Loaded spaCy model: {spacy_model}\")\n        except Exception as e:\n            logging.warning(f\"spaCy model load failed ({e}); falling back to token-only processing.\")\n            nlp = None\n\n    lemmas_list = []\n    texts = df[\"text\"].tolist()\n\n    if nlp is not None:\n        # prefer nlp.pipe with safe parameters; avoid n_process unless explicitly supported\n        for doc in nlp.pipe(texts, batch_size=8):\n            lemma_tokens = [tok.lemma_.lower() for tok in doc if tok.is_alpha]\n            lemmas_list.append(\" \".join(lemma_tokens))\n    else:\n        # simple fallback tokenization (alphabetic only)\n        for t in texts:\n            lemmas_list.append(\" \".join(re.findall(r\"[A-Za-z]+\", t.lower())))\n\n    df_out = pd.DataFrame({\n        \"chapter\": df[\"chapter\"].astype(int),\n        \"marker\": df.get(\"marker\", df[\"chapter\"].astype(str)),\n        \"lemmas_str\": lemmas_list,\n        \"word_count\": df[\"word_count\"].astype(int)\n    })\n\n    df_out.to_csv(CACHE_PATH, index=False, encoding=\"utf-8\")\n    logging.info(f\"Wrote lemma cache to {CACHE_PATH} ({len(df_out)} rows).\")\n    return df_out\n\ndef load_or_compute_lemmas(chap_df: pd.DataFrame, force_recompute: bool = False) -> pd.DataFrame:\n    if CACHE_PATH.exists() and not force_recompute:\n        try:\n            df_cached = pd.read_csv(CACHE_PATH, encoding=\"utf-8\")\n            df_cached[\"chapter\"] = pd.to_numeric(df_cached[\"chapter\"], errors=\"coerce\").fillna(0).astype(int)\n            df_cached[\"word_count\"] = pd.to_numeric(df_cached.get(\"word_count\", 0), errors=\"coerce\").fillna(0).astype(int)\n            logging.info(f\"Loaded cached lemmas from {CACHE_PATH}\")\n            return df_cached\n        except Exception as e:\n            logging.warning(f\"Failed to load cached lemmas ({e}), will recompute.\")\n    # compute using spaCy if available\n    try:\n        return compute_lemmas_df(chap_df, use_spacy=True)\n    except Exception as e:\n        logging.warning(f\"spaCy computation failed: {e} — falling back to tokenization-only.\")\n        return compute_lemmas_df(chap_df, use_spacy=False)\n\n# set to True if you want to force recompute in Quarto run (e.g., for reproducible build)\nFORCE_RECOMPUTE_LEMMAS = False\nchap_lemmas_df = load_or_compute_lemmas(chap_df, force_recompute=FORCE_RECOMPUTE_LEMMAS)\n\n```\n\n```{python}\n#| label: widgets_termfreq_lemmatized\n#| echo: false\n#| eval: true\n#| cache: true\n#| warning: false\n#| error: false\n\n\"\"\"\nInteractive lemmatized term-frequency widget for Binder/Jupyter.\n\nSingle-word queries only (lemmatized forms). For Quarto static renders we print\ninstructions explaining how to use the widget in Binder.\n\"\"\"\n\nimport logging\nimport re\nimport pandas as pd\nfrom collections import Counter\n\n# Defensive checks\nif \"chap_lemmas_df\" not in globals():\n    raise RuntimeError(\"chap_lemmas_df not found — run the chapter/lemma cells before this one.\")\n\n# --------------------------------------------------\n# Precompute per-chapter lemma counters (single-word only)\n# --------------------------------------------------\n_chapter_counters = []\n_chapter_word_counts = chap_lemmas_df[\"word_count\"].astype(int).tolist()\n\n# Build list of lemma strings to keep ordering stable for plotting\n_lemma_strings_list = chap_lemmas_df[\"lemmas_str\"].fillna(\"\").astype(str).tolist()\n\nfor s in _lemma_strings_list:\n    if not s:\n        _chapter_counters.append(Counter())\n    else:\n        _chapter_counters.append(Counter(s.split()))\n\n# Helper functions\ndef lemmatize_query_simple(q: str):\n    q = q.strip().lower()\n    if not q:\n        return []\n    return re.findall(r\"[a-z]+\", q)\n\ndef count_single_lemma(word: str):\n    \"\"\"\n    Count a single lemmatized word per chapter.\n    Extremely fast: Counter lookup per chapter.\n    \"\"\"\n    w = word.strip().lower()\n    if not w:\n        return [0] * len(_chapter_counters)\n    return [c.get(w, 0) for c in _chapter_counters]\n\ndef make_plot_df(lemmas_str_df: pd.DataFrame, query: str, normalize: bool = False):\n    lemmas = lemmatize_query_simple(query)\n    if len(lemmas) != 1:\n        # enforce single-word queries — return empty DataFrame to signal nothing to plot\n        return pd.DataFrame()\n\n    counts = count_single_lemma(lemmas[0])\n\n    plot_df = pd.DataFrame({\n        \"chapter\": lemmas_str_df[\"chapter\"].astype(int).tolist(),\n        \"count\": counts,\n        \"marker\": lemmas_str_df.get(\"marker\", lemmas_str_df[\"chapter\"].astype(str)).tolist(),\n        \"word_count\": lemmas_str_df[\"word_count\"].astype(int).tolist()\n    }).sort_values(\"chapter\").reset_index(drop=True)\n\n    plot_df = plot_df[plot_df[\"chapter\"] > 0].copy()  # exclude preface / aggregate (chapter==0)\n\n    if plot_df.empty:\n        return plot_df  # empty DataFrame to signal nothing to plot\n\n    if normalize:\n        plot_df[\"norm_per_1k\"] = plot_df[\"count\"] / (plot_df[\"word_count\"].replace({0: 1}) / 1000.0)\n    return plot_df\n\n# Try to import interactive dependencies\nINTERACTIVE = False\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display, HTML, clear_output\n    import plotly.express as px\n    INTERACTIVE = True\nexcept Exception:\n    INTERACTIVE = False\n\nif not INTERACTIVE:\n    # Non-interactive build: show brief instructions for Binder usage\n    print(\"This page includes an interactive term-frequency widget. To use it, launch this Quarto project in Binder (see project README).\")\n    print(\"In Binder's Jupyter interface, open this notebook and type a single lemmatized word (e.g., 'vampire'), then click Plot.\")\nelse:\n    # Build UI: text input + normalize checkbox + plot area + helpful instructions\n    term_input = widgets.Text(\n        value=\"\",\n        placeholder=\"Type a single word (e.g. vampire)\",\n        description=\"Term:\",\n        layout=widgets.Layout(width=\"60%\")\n    )\n\n    normalize_chk = widgets.Checkbox(\n        value=False,\n        description=\"Normalize (per 1k words)\",\n        indent=False\n    )\n\n    plot_btn = widgets.Button(description=\"Plot\", button_style=\"primary\")\n    out = widgets.Output(layout=widgets.Layout(width=\"100%\"))\n\n    def render_plot_for_query(query_text, normalize=False):\n        \"\"\"\n        Build and show a plotly figure for the given single-word query_text.\n        Returns True if successful, False if nothing to show.\n        \"\"\"\n        q = query_text.strip()\n        if not q:\n            with out:\n                clear_output(wait=True)\n                print(\"Please enter a single lemmatized word to plot.\")\n            return False\n\n        lemmas = lemmatize_query_simple(q)\n        if len(lemmas) != 1:\n            with out:\n                clear_output(wait=True)\n                print(\"Please enter a single word (phrases are not supported).\")\n            return False\n\n        plot_df = make_plot_df(chap_lemmas_df, q, normalize=normalize)\n        with out:\n            clear_output(wait=True)\n            if plot_df.empty:\n                print(f\"No occurrences of '{q}' found in chapter texts.\")\n                return False\n\n            # Pick column to plot\n            ycol = \"norm_per_1k\" if normalize and \"norm_per_1k\" in plot_df.columns else \"count\"\n            ylabel = \"Occurrences per 1,000 words\" if ycol == \"norm_per_1k\" else \"Raw occurrences\"\n\n            try:\n                fig = px.line(\n                    plot_df,\n                    x=\"chapter\",\n                    y=ycol,\n                    markers=True,\n                    title=f\"Frequency of '{q}' by chapter\",\n                    labels={\"chapter\": \"Chapter\", ycol: ylabel},\n                    hover_data=[\"marker\", \"chapter\", ycol]\n                )\n                fig.update_traces(mode=\"lines+markers\")\n                fig.update_layout(height=420, margin=dict(l=40, r=20, t=60, b=40), xaxis=dict(dtick=1))\n                fig.show()\n                return True\n            except Exception as e:\n                print(\"Plotly failed to render the figure:\", e)\n                return False\n\n    # Wire events: button click + Enter key on input triggers the same behavior\n    def on_click_btn(b):\n        render_plot_for_query(term_input.value, normalize_chk.value)\n\n    plot_btn.on_click(on_click_btn)\n\n    # bind Enter key if Text has on_submit (ipykernel >= ?)\n    try:\n        term_input.on_submit(lambda widget: render_plot_for_query(widget.value, normalize_chk.value))\n    except Exception:\n        # fallback: helpful tip in UI; user can click Plot\n        pass\n\n    # render the UI\n    display(HTML(\"<h4>Search term frequency across chapters (lemmatized)</h4>\"))\n    display(widgets.HBox([term_input, plot_btn, normalize_chk]))\n    display(out)\n\n    # Optionally show a short hint / example\n    with out:\n        print(\"Type a word (e.g., 'vampire') and click Plot. Press Enter in the textbox where supported.\")\n\n```\n\n---\nformat: html\nexecute:\n  echo: false\n  enabled: true\n---\n\n<div style=\"max-width:900px\">\n  <p>Enter a single **lemmatized** word (no multi-word phrases). Example: <code>vampire</code></p>\n\n  <label for=\"lemma\">Lemma:</label>\n  <input id=\"lemma\" placeholder=\"vampire\" style=\"width:220px\" />\n  <button id=\"plotBtn\">Plot</button>\n  <label style=\"margin-left:12px\"><input type=\"checkbox\" id=\"normalize\" /> Normalize (per 1k words)</label>\n\n  <!-- status is an ARIA live region -->\n  <div id=\"status\" style=\"margin-top:8px;color:#666\" aria-live=\"polite\" role=\"status\">Status: initializing...</div>\n  <div id=\"plotArea\" style=\"margin-top:12px\"></div>\n\n  <p style=\"color:#666; margin-top:14px; font-size:0.95em\">\n    This lightweight page parses a precomputed CSV in-browser and produces an accessible plot + numeric table. If you need spaCy or NER, run the original notebook (Binder/Colab).\n  </p>\n</div>\n\n<script type=\"module\">\n// ---------- Configuration ----------\nconst CSV_CANDIDATES = [\n  \"https://iulibscholcomm.github.io/dracula/data/chap_lemmas.csv\",\n  \"/dracula/data/chap_lemmas.csv\",\n  \"/data/chap_lemmas.csv\",\n  \"data/chap_lemmas.csv\",\n  \"chap_lemmas.csv\"\n];\n\nconst statusEl = document.getElementById(\"status\");\nconst plotEl = document.getElementById(\"plotArea\");\nconst plotBtn = document.getElementById(\"plotBtn\");\nconst lemmaInput = document.getElementById(\"lemma\");\nconst normalizeBox = document.getElementById(\"normalize\");\n\nlet cachedChapters = null;\n\n// ---------- Fetch & parse CSV ----------\nasync function fetchCsvCandidates(candidates) {\n  for (const url of candidates) {\n    statusEl.textContent = `Status: attempting to fetch ${url} ...`;\n    try {\n      const r = await fetch(url);\n      if (!r.ok) continue;\n      const text = await r.text();\n      statusEl.textContent = `Status: loaded ${url}`;\n      return { url, text };\n    } catch (e) {\n      console.debug(\"fetch failed\", url, e);\n      continue;\n    }\n  }\n  throw new Error(\"All CSV fetch attempts failed.\");\n}\n\n// Minimal CSV parser with quoted-field support\nfunction parseCSV(text) {\n  const rows = [];\n  let cur = \"\", inQuotes = false, row = [];\n  for (let i = 0; i < text.length; i++) {\n    const ch = text[i];\n    const nch = text[i+1];\n    if (ch === '\"') {\n      if (inQuotes && nch === '\"') { cur += '\"'; i++; } // escaped quote\n      else inQuotes = !inQuotes;\n    } else if (ch === ',' && !inQuotes) {\n      row.push(cur); cur = \"\";\n    } else if ((ch === '\\n' || ch === '\\r') && !inQuotes) {\n      if (ch === '\\r' && nch === '\\n') { i++; }\n      row.push(cur);\n      rows.push(row);\n      row = [];\n      cur = \"\";\n    } else {\n      cur += ch;\n    }\n  }\n  if (cur !== \"\" || row.length > 0) { row.push(cur); rows.push(row); }\n  return rows;\n}\n\nfunction csvRowsToObjects(rows) {\n  if (!rows || rows.length === 0) return [];\n  const header = rows[0].map(h => h.trim());\n  const objs = [];\n  for (let i = 1; i < rows.length; i++) {\n    if (rows[i].length === 1 && rows[i][0].trim() === \"\") continue;\n    const obj = {};\n    for (let j = 0; j < header.length; j++) {\n      obj[header[j]] = rows[i][j] !== undefined ? rows[i][j] : \"\";\n    }\n    objs.push(obj);\n  }\n  return objs;\n}\n\n// ---------- Data prep ----------\nfunction buildChapterCounters(objs) {\n  return objs.map(o => {\n    const chap = Number(o.chapter) || 0;\n    const lemmas = (o.lemmas_str || \"\").trim();\n    const word_count = Number(o.word_count) || 0;\n    const tokens = lemmas ? lemmas.split(/\\s+/) : [];\n    const freq = {};\n    for (const t of tokens) {\n      const key = t.toLowerCase();\n      freq[key] = (freq[key] || 0) + 1;\n    }\n    return { chapter: chap, freq, word_count };\n  });\n}\n\nfunction computeCounts(chapters, query, normalizeFlag) {\n  const q = query.trim().toLowerCase();\n  const raw = chapters.map(c => c.freq[q] || 0);\n  const words = chapters.map(c => c.word_count || 0);\n  const norm = normalizeFlag ? raw.map((v,i) => {\n    const wc = words[i] || 1;\n    return (v / wc) * 1000.0;\n  }) : null;\n  return { raw, norm, words };\n}\n\n// ---------- Trend calc ----------\nfunction computeTrend(series) {\n  const n = series.length;\n  if (n < 2) return { text: \"Not enough points to compute trend.\", slope: 0, r: 0 };\n  const x = Array.from({length:n}, (_,i)=>i+1);\n  const mean = arr => arr.reduce((a,b)=>a+b,0)/arr.length;\n  const mx = mean(x), my = mean(series);\n  let num = 0, denx = 0, deny = 0;\n  for (let i=0;i<n;i++){\n    num += (x[i]-mx)*(series[i]-my);\n    denx += (x[i]-mx)*(x[i]-mx);\n    deny += (series[i]-my)*(series[i]-my);\n  }\n  const slope = denx === 0 ? 0 : num/denx;\n  const r = (denx*deny === 0) ? 0 : (num / Math.sqrt(denx*deny));\n  let text = \"\";\n  if (Math.abs(r) >= 0.4) {\n    text = slope > 0 ? `Increasing trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`\n                     : `Decreasing trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`;\n  } else if (Math.abs(r) >= 0.2) {\n    text = slope > 0 ? `Mild upward trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`\n                     : `Mild downward trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`;\n  } else {\n    text = `No clear trend (slope=${slope.toFixed(3)}, r=${r.toFixed(2)}).`;\n  }\n  return { text, slope, r };\n}\n\n// ---------- SVG plot renderer ----------\nfunction renderSvgPlot(values, opts={width:900,height:260, stroke:'#1b4f72'}) {\n  const W = opts.width, H = opts.height;\n  const pad = {left:40,right:12,top:18,bottom:36};\n  const n = Math.max(values.length, 1);\n  const maxv = Math.max(...values, 1);\n  const minv = Math.min(...values, 0);\n  const span = (maxv - minv) || 1;\n  const x = i => pad.left + ( (n===1) ? (W - pad.left - pad.right)/2 : (i/(n-1))*(W - pad.left - pad.right) );\n  const y = v => pad.top + (1 - (v - minv)/span)*(H - pad.top - pad.bottom);\n\n  const pts = values.map((v,i)=>`${x(i)},${y(v)}`).join(' ');\n\n  const svg = document.createElementNS(\"http://www.w3.org/2000/svg\",\"svg\");\n  svg.setAttribute(\"viewBox\", `0 0 ${W} ${H}`);\n  svg.setAttribute(\"width\", \"100%\");\n  svg.setAttribute(\"height\", H);\n  svg.setAttribute(\"role\",\"img\");\n  svg.style.maxWidth = \"100%\";\n  svg.style.outline = \"none\";\n\n  const ny = 4;\n  for (let i=0;i<=ny;i++){\n    const val = minv + (i/ny)*span;\n    const yPos = y(val);\n    const line = document.createElementNS(svg.namespaceURI,'line');\n    line.setAttribute('x1', pad.left);\n    line.setAttribute('x2', W - pad.right);\n    line.setAttribute('y1', yPos);\n    line.setAttribute('y2', yPos);\n    line.setAttribute('stroke', '#eee');\n    line.setAttribute('stroke-width','1');\n    svg.appendChild(line);\n\n    const tx = document.createElementNS(svg.namespaceURI,'text');\n    tx.setAttribute('x', 8);\n    tx.setAttribute('y', yPos - 2);\n    tx.setAttribute('font-size','11');\n    tx.setAttribute('fill','#333');\n    tx.textContent = Number(val).toFixed(0);\n    svg.appendChild(tx);\n  }\n\n  const pl = document.createElementNS(svg.namespaceURI,'polyline');\n  pl.setAttribute('points', pts);\n  pl.setAttribute('fill','none');\n  pl.setAttribute('stroke', opts.stroke || '#000');\n  pl.setAttribute('stroke-width','2');\n  svg.appendChild(pl);\n\n  for (let i=0;i<values.length;i++){\n    const c = document.createElementNS(svg.namespaceURI,'circle');\n    c.setAttribute('cx', x(i));\n    c.setAttribute('cy', y(values[i]));\n    c.setAttribute('r','3');\n    c.setAttribute('fill', opts.stroke || '#000');\n    svg.appendChild(c);\n  }\n\n  for (let i=0;i<values.length;i++){\n    if (values.length > 30 && (i % Math.ceil(values.length/20) !== 0)) continue;\n    const tx = document.createElementNS(svg.namespaceURI,'text');\n    tx.setAttribute('x', x(i));\n    tx.setAttribute('y', H - 8);\n    tx.setAttribute('font-size','10');\n    tx.setAttribute('text-anchor','middle');\n    tx.setAttribute('fill','#333');\n    tx.textContent = (i+1).toString();\n    svg.appendChild(tx);\n  }\n\n  return svg;\n}\n\n// ---------- Accessible report builder ----------\nfunction buildAccessibleReport(container, query, countsRaw, countsNorm, wordCounts, normalizeFlag, trendText) {\n  container.innerHTML = \"\";\n\n  const trendP = document.createElement(\"p\");\n  trendP.style.fontStyle = \"italic\";\n  trendP.textContent = trendText || \"\";\n  container.appendChild(trendP);\n\n  const total = countsRaw.reduce((a,b)=>a+b,0);\n  const maxv = Math.max(...countsRaw);\n  const maxCh = countsRaw.map((v,i)=>v===maxv?(i+1):null).filter(Boolean).join(\", \") || \"N/A\";\n  const minv = Math.min(...countsRaw);\n  const minCh = countsRaw.map((v,i)=>v===minv?(i+1):null).filter(Boolean).join(\", \") || \"N/A\";\n  const summaryP = document.createElement(\"p\");\n  summaryP.textContent = `${countsRaw.length} chapters. Total occurrences (raw): ${total}. Highest raw count: ${maxv} (chapter(s) ${maxCh}). Lowest raw count: ${minv} (chapter(s) ${minCh}).`;\n  container.appendChild(summaryP);\n\n  const csvLines = [\"chapter,word_count,raw_count\" + (countsNorm ? \",normalized_per_1k\" : \"\")];\n  for (let i=0;i<countsRaw.length;i++){\n    const chap = i+1;\n    const wc = wordCounts[i] || \"\";\n    const raw = countsRaw[i] || 0;\n    const norm = countsNorm ? countsNorm[i] : \"\";\n    csvLines.push([chap, wc, raw, norm].filter((_,idx)=> idx<3 || countsNorm).join(\",\"));\n  }\n  const csvBlob = new Blob([csvLines.join(\"\\n\")], {type: \"text/csv\"});\n  const url = URL.createObjectURL(csvBlob);\n  const a = document.createElement(\"a\");\n  a.href = url;\n  a.download = `dracula_${query}_chapter_counts.csv`;\n  a.textContent = \"Download chapter counts (CSV)\";\n  a.style.display = \"inline-block\";\n  a.style.margin = \"6px 0\";\n  container.appendChild(a);\n\n  const table = document.createElement(\"table\");\n  table.style.width = \"100%\";\n  table.style.borderCollapse = \"collapse\";\n  table.style.marginTop = \"8px\";\n  table.setAttribute(\"role\",\"table\");\n\n  const caption = document.createElement(\"caption\");\n  caption.textContent = `Chapter counts for lemma \"${query}\"`;\n  caption.style.textAlign = \"left\";\n  caption.style.fontWeight = \"600\";\n  table.appendChild(caption);\n\n  const thead = document.createElement(\"thead\");\n  const thr = document.createElement(\"tr\");\n  [\"Chapter\",\"Word count\",\"Raw occurrences\", normalizeFlag ? \"Normalized per 1k words\" : null].forEach(h=>{\n    if (!h) return;\n    const th = document.createElement(\"th\");\n    th.scope = \"col\";\n    th.textContent = h;\n    th.style.borderBottom = \"1px solid #ccc\";\n    th.style.padding = \"6px 4px\";\n    thr.appendChild(th);\n  });\n  thead.appendChild(thr);\n  table.appendChild(thead);\n\n  const tbody = document.createElement(\"tbody\");\n  for (let i=0;i<countsRaw.length;i++){\n    const tr = document.createElement(\"tr\");\n    const th = document.createElement(\"th\");\n    th.scope = \"row\";\n    th.textContent = (i+1).toString();\n    th.style.padding = \"6px 4px\";\n    tr.appendChild(th);\n\n    const tdW = document.createElement(\"td\");\n    tdW.textContent = wordCounts[i] || \"\";\n    tdW.style.padding = \"6px 4px\";\n    tr.appendChild(tdW);\n\n    const tdR = document.createElement(\"td\");\n    tdR.textContent = countsRaw[i] || 0;\n    tdR.style.padding = \"6px 4px\";\n    tr.appendChild(tdR);\n\n    if (normalizeFlag) {\n      const tdN = document.createElement(\"td\");\n      tdN.textContent = (countsNorm && countsNorm[i]!==undefined && countsNorm[i]!==null) ? Number(countsNorm[i]).toFixed(2) : \"\";\n      tdN.style.padding = \"6px 4px\";\n      tr.appendChild(tdN);\n    }\n\n    tbody.appendChild(tr);\n  }\n  table.appendChild(tbody);\n  container.appendChild(table);\n}\n\n// ---------- Main UI flow ----------\nasync function initializeData() {\n  try {\n    const res = await fetchCsvCandidates(CSV_CANDIDATES);\n    const rows = parseCSV(res.text);\n    const objs = csvRowsToObjects(rows);\n\n    if (objs.length === 0 || !('lemmas_str' in objs[0])) {\n      statusEl.textContent = \"Status: CSV missing expected columns (chapter, lemmas_str, word_count).\";\n      throw new Error(\"CSV parse/format issue\");\n    }\n\n    cachedChapters = buildChapterCounters(objs);\n    statusEl.textContent = `Status: ready (using ${res.url}). Enter a lemmatized word and click Plot.`;\n  } catch (e) {\n    console.error(\"Data init error:\", e);\n    statusEl.textContent = \"Status: failed to load or parse chap_lemmas.csv. Check console.\";\n  }\n}\n\nasync function plotLemma(query, normalizeFlag) {\n  if (!cachedChapters) {\n    statusEl.textContent = \"Status: data not initialized.\";\n    return;\n  }\n  if (!query || !query.trim()) { alert(\"Enter a lemmatized single word (e.g., 'vampire').\"); return; }\n\n  statusEl.textContent = \"Status: computing counts & plotting...\";\n  try {\n    const { raw, norm, words } = computeCounts(cachedChapters, query, normalizeFlag);\n    const seriesForTrend = normalizeFlag ? norm : raw;\n    const trend = computeTrend(seriesForTrend.map(v => Number(v || 0)));\n\n    plotEl.innerHTML = \"\";\n    const heading = document.createElement(\"div\");\n    heading.textContent = `Frequency of '${query}' (lemmatized) — n_chapters=${raw.length}`;\n    heading.style.fontWeight = \"600\";\n    heading.style.marginBottom = \"6px\";\n    plotEl.appendChild(heading);\n\n    const svgValues = (normalizeFlag && norm) ? norm : raw;\n    const svg = renderSvgPlot(svgValues, {width:900,height:260, stroke:'#1b4f72'});\n    svg.setAttribute(\"tabindex\", \"0\");\n    const desc = document.createElement(\"div\");\n    desc.style.marginTop = \"8px\";\n    desc.setAttribute(\"role\",\"region\");\n    desc.setAttribute(\"aria-live\",\"polite\");\n    desc.setAttribute(\"id\", \"desc-\" + Math.random().toString(36).slice(2,8));\n    svg.setAttribute(\"aria-describedby\", desc.id);\n    plotEl.appendChild(svg);\n    plotEl.appendChild(desc);\n\n    buildAccessibleReport(desc, query, raw, norm, words, normalizeFlag, trend.text);\n\n    statusEl.textContent = \"Status: done.\";\n  } catch (err) {\n    console.error(\"Plot error:\", err);\n    statusEl.textContent = \"Status: error during plotting — see console.\";\n    plotEl.innerHTML = `<pre style=\"color:red\">Error: ${String(err)}</pre>`;\n  }\n}\n\n// UI wiring\nplotBtn.addEventListener(\"click\", () => plotLemma(lemmaInput.value.trim(), normalizeBox.checked));\nlemmaInput.addEventListener(\"keydown\", (e) => { if (e.key === \"Enter\") plotBtn.click(); });\n\n// init\ninitializeData();\n</script>\n\n\n\n\n```{python}\n#| label: ner_spacy_per_chapter\n#| echo: false\n#| eval: false\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n\ntry:\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")\n    logging.info(\"spaCy model loaded for NER.\")\nexcept Exception as e:\n    logging.warning(f\"spaCy model not available ({e}). Skipping NER extraction and writing empty CSVs.\")\n    nlp = None\n\nOUTPUT_PLACES_BY_CH = DATA_DIR / \"places_by_chapter_raw.csv\"\nOUTPUT_PLACE_COUNTS = DATA_DIR / \"place_counts_raw.csv\"\nOUTPUT_PLACES_PER_CH = DATA_DIR / \"places_per_chapter_raw.csv\"\nOUTPUT_PLACES_JSON = DATA_DIR / \"places_per_chapter.json\"\n\nrows = []\nif nlp:\n    # conservative heuristics\n    DIRECTIONAL = set([\"north\",\"south\",\"east\",\"west\",\"northern\",\"southern\",\"eastern\",\"western\",\"left\",\"right\",\"up\",\"down\",\"here\",\"there\"])\n    SHORT_MIN_CHARS = 3\n    _initials_re = re.compile(r'^(?:[A-Z]\\.){1,3}$')\n    _digits_re = re.compile(r'\\d')\n\n    for _, row in chap_df.iterrows():\n        chap = row['chapter']\n        text = row['text'] or \"\"\n        doc = nlp(text)\n        for ent in doc.ents:\n            if ent.label_ not in (\"GPE\",\"LOC\",\"FAC\"):\n                continue\n            raw = ent.text.strip()\n            candidate = re.sub(r'\\s+', ' ', raw).strip(\" \\t\\n\\r,.;:()[]\\\"'\")\n            low = candidate.lower()\n            if len(candidate) < SHORT_MIN_CHARS:\n                continue\n            if _initials_re.match(candidate):\n                continue\n            if _digits_re.search(candidate):\n                continue\n            if low in DIRECTIONAL:\n                continue\n            if low in {\"and\",\"the\",\"that\",\"this\",\"there\",\"here\",\"it\",\"i\",\"we\",\"you\",\"he\",\"she\",\"they\"}:\n                continue\n            # do not over-normalize proper names: keep as-is but strip extra spaces\n            rows.append({\"place\": candidate, \"chapter\": int(chap) if not pd.isna(chap) else 0})\n\n# write outputs (even if empty)\nplaces_by_ch = pd.DataFrame(rows, columns=[\"place\",\"chapter\"])\nplaces_by_ch.to_csv(OUTPUT_PLACES_BY_CH, index=False, encoding=\"utf-8\")\nif not places_by_ch.empty:\n    place_counts = places_by_ch['place'].value_counts().reset_index()\n    place_counts.columns = ['place','count']\n    place_counts.to_csv(OUTPUT_PLACE_COUNTS, index=False, encoding=\"utf-8\")\n\n    places_per_ch = places_by_ch.groupby(['place','chapter'], dropna=False).size().reset_index(name='count')\n    places_per_ch.to_csv(OUTPUT_PLACES_PER_CH, index=False, encoding=\"utf-8\")\n    places_per_ch.to_json(OUTPUT_PLACES_JSON, orient=\"records\", force_ascii=False)\nelse:\n    pd.DataFrame(columns=[\"place\",\"count\"]).to_csv(OUTPUT_PLACE_COUNTS, index=False, encoding=\"utf-8\")\n    pd.DataFrame(columns=[\"place\",\"chapter\",\"count\"]).to_csv(OUTPUT_PLACES_PER_CH, index=False, encoding=\"utf-8\")\n\nlogging.info(\"NER extraction complete — wrote CSVs (or empty skeletons).\")\n```\n\n```{python}\n#| label: prepare_geodata_for_plotly\n#| echo: false\n#| eval: false\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n\nGEOCODE_CSV = DATA_DIR / \"places_geocoded.csv\"         # produced by a separate geocoding step\nPLACES_PER_CH_RAW = DATA_DIR / \"places_per_chapter_raw.csv\"\nOUT_CSV = DATA_DIR / \"places_per_chapter.csv\"\nSUSPECT_CSV = DATA_DIR / \"geocode_suspect_manual_review_needed.csv\"\n\nif not GEOCODE_CSV.exists():\n    logging.warning(f\"{GEOCODE_CSV} not found. Geocoding step must run separately. Skipping geodata preparation.\")\nelse:\n    geocoded = pd.read_csv(GEOCODE_CSV)\n    if not PLACES_PER_CH_RAW.exists():\n        raise FileNotFoundError(\"Run ner_spacy_per_chapter to produce places_per_chapter_raw.csv before preparing geodata.\")\n    places_per_ch = pd.read_csv(PLACES_PER_CH_RAW)\n\n    geo_counts = places_per_ch.merge(geocoded, on=\"place\", how=\"left\")\n    missing_coords = geo_counts[geo_counts['lat'].isna() | geo_counts['lon'].isna()]\n    if not missing_coords.empty:\n        logging.warning(f\"{len(missing_coords)} place x chapter rows missing coordinates. See data/geocode_missing_coords.csv\")\n        missing_coords.to_csv(DATA_DIR / \"geocode_missing_coords.csv\", index=False, encoding=\"utf-8\")\n\n    suspect_rows = []\n    euro_keywords = {\"Transylvania\",\"Bistritz\",\"Bistrita\",\"Romania\",\"Bukovina\",\"Klausenburgh\",\"Borgo\"}\n    for _, r in geo_counts.iterrows():\n        place = r.get(\"place\")\n        lat = r.get(\"lat\")\n        lon = r.get(\"lon\")\n        if pd.isna(lat) or pd.isna(lon):\n            continue\n        if any(k.lower() in str(place).lower() for k in euro_keywords):\n            try:\n                if not (-30 <= float(lon) <= 60):\n                    suspect_rows.append({**r, \"reason\":\"euro_name_geocoded_outside_europe\"})\n            except Exception:\n                continue\n    if suspect_rows:\n        pd.DataFrame(suspect_rows).to_csv(SUSPECT_CSV, index=False, encoding=\"utf-8\")\n        logging.info(f\"Wrote {len(suspect_rows)} suspect geocodes to {SUSPECT_CSV}\")\n\n    geo_counts = geo_counts.dropna(subset=[\"lat\",\"lon\"]).copy()\n    geo_counts['lat'] = pd.to_numeric(geo_counts['lat'], errors='coerce')\n    geo_counts['lon'] = pd.to_numeric(geo_counts['lon'], errors='coerce')\n    geo_counts['count'] = pd.to_numeric(geo_counts['count'], errors='coerce').fillna(0).astype(int)\n    geo_counts['chapter'] = pd.to_numeric(geo_counts['chapter'], errors='coerce').fillna(0).astype(int)\n\n    agg_all = geo_counts.groupby(['place','lat','lon'], as_index=False)['count'].sum()\n    agg_all['chapter'] = 0\n\n    final = pd.concat([agg_all, geo_counts], ignore_index=True, sort=False)\n    final = final[['place','lat','lon','chapter','count']].sort_values(['chapter','count'], ascending=[True,False])\n    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n    logging.info(f\"Wrote merged per-chapter geodata to {OUT_CSV} ({len(final)} rows).\")\n\n```\n```{python}\n#| label: plotly_bubble_map\n#| echo: true\n#| eval: true\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n#| code-fold: true\n\nDATA_CSV = DATA_DIR / \"places_per_chapter.csv\"\nif not DATA_CSV.exists():\n    logging.warning(\"places_per_chapter.csv not present; run prepare_geodata_for_plotly first.\")\nelse:\n    import plotly.express as px\n\n    df = pd.read_csv(DATA_CSV)\n    df = df.dropna(subset=['lat','lon'])\n    df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(int)\n    df['chapter'] = pd.to_numeric(df['chapter'], errors='coerce').fillna(0).astype(int)\n\n    # compute numeric chapter order for animation and a display label\n    chapter_order = sorted(df['chapter'].unique().tolist())\n    df['chapter_label'] = df['chapter'].apply(lambda c: \"All chapters\" if c == 0 else f\"Chapter {int(c)}\")\n\n    max_count = df['count'].max() if not df.empty else 1\n    desired_max_size = 60\n    sizeref = 2.0 * max_count / (desired_max_size ** 2) if max_count > 0 else 1\n\n    fig = px.scatter_geo(df,\n                         lat='lat', lon='lon',\n                         size='count',\n                         hover_name='place',\n                         hover_data={'count': True, 'chapter': True, 'chapter_label': True},\n                         animation_frame='chapter',           # numeric ensures correct ordering\n                         category_orders={'chapter': chapter_order},\n                         projection='natural earth',\n                         title=\"Geographic distribution of place references in Dracula (bubble size = mentions)\")\n    # set explicit sizeref on traces/frames\n    if hasattr(fig, \"frames\") and fig.frames:\n        for frame in fig.frames:\n            for trace in frame.data:\n                try:\n                    trace.marker.sizeref = sizeref\n                except Exception:\n                    pass\n    for trace in fig.data:\n        try:\n            trace.marker.sizeref = sizeref\n        except Exception:\n            pass\n\n    fig.update_layout(height=600, margin=dict(l=10, r=10, t=60, b=10), template=\"plotly_white\")\n    # control animation speed gracefully (if controls exist)\n    try:\n        fig.layout.updatemenus[0].buttons[0].args[1]['frame']['duration'] = 800\n        fig.layout.updatemenus[0].buttons[0].args[1]['transition']['duration'] = 300\n    except Exception:\n        pass\n\n    fig.show()\n\n```\n\n\n```{python}\n#| label: sentiment_vader\n#| echo: false\n#| eval: true\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n\n# ensure VADER resource available; try to download quietly if missing\ntry:\n    nltk.data.find(\"sentiment/vader_lexicon.zip\")\nexcept LookupError:\n    logging.info(\"Downloading VADER lexicon (nltk).\")\n    nltk.download(\"vader_lexicon\", quiet=True)\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n# compute raw VADER compound score per chapter (works for full chapter text)\nchap_df = chap_df.copy()\nchap_df['vader_compound'] = chap_df['text'].apply(lambda t: sid.polarity_scores(t)['compound'] if isinstance(t, str) else 0.0)\n\n# compute word counts and guard against zero\nchap_df['word_count'] = chap_df['text'].str.split().str.len().fillna(0).astype(int)\nwc = chap_df['word_count'].replace({0: 1})   # safe divisor\nchap_df['sentiment_per_1k_words'] = chap_df['vader_compound'] / (wc / 1000.0)\n\n# persist for later use or external analysis\n( DATA_DIR / \"sentiment_by_chapter.csv\" ).write_text(chap_df[['chapter','marker','word_count','vader_compound','sentiment_per_1k_words']].to_csv(index=False, encoding=\"utf-8\"), encoding=\"utf-8\")\nlogging.info(\"Wrote data/sentiment_by_chapter.csv\")\n```\n```{python}\n#| label: sentiment_line_plot_normalized\n#| echo: true\n#| eval: true\n#| cache: true\n#| freeze: auto\n#| warning: false\n#| error: false\n#| code-fold: true\n\nimport plotly.express as px\n\nplot_df = chap_df[chap_df[\"chapter\"] > 0].sort_values(\"chapter\").copy()\nif plot_df.empty:\n    print(\"No chapter data available to plot.\")\nelse:\n    plot_df[\"chapter\"] = pd.to_numeric(plot_df[\"chapter\"], errors=\"coerce\")\n\n    fig = px.line(plot_df, x=\"chapter\", y=\"sentiment_per_1k_words\", markers=True,\n                  title=\"Length-normalized sentiment trajectory across Dracula\",\n                  labels={\"chapter\":\"Chapter\", \"sentiment_per_1k_words\":\"Sentiment per 1,000 words (VADER)\"},\n                  hover_data={\"marker\":True, \"sentiment_per_1k_words\":\":.3f\"})\n    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Neutral (0)\", annotation_position=\"bottom right\")\n    fig.update_layout(margin=dict(l=60, r=20, t=60, b=60), height=420, xaxis=dict(dtick=1, tickmode=\"linear\"), template=\"plotly_white\")\n    fig.show()\n\n```","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":false,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":true,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","html-q-tags":true,"section-divs":true,"toc":true,"number-sections":true,"output-file":"99-text-analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en-US","fig-responsive":true,"quarto-version":"1.9.19","engines":[{"path":"C:\\Users\\amazel\\AppData\\Local\\Programs\\Quarto\\share\\extension-subtrees\\julia-engine\\_extensions\\julia-engine\\julia-engine.js"}],"theme":["cosmo","brand"],"anchor-sections":true,"tabsets":true,"respect-user-color-scheme":true,"smooth-scroll":true,"page-layout":"article","title-block-style":"default","lightbox":true,"fig-cap-location":"margin","code-summary":"Show / Hide Code","code-copy":true,"code-block-border-left":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}